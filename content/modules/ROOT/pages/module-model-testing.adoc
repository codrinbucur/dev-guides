= Model Testing - 30 minutes
:imagesdir: ../assets/images
:sectnums:

++++
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3HTRSDJ3M4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-3HTRSDJ3M4');
</script>
++++

== Goals of this lab

Parasol's developers are always expected to thoroughly test their code. In this exercise, you will explore the unique challenges and techniques involved in testing AI models and applications, which differ significantly from traditional enterprise applications. Traditional apps often have deterministic outputs, whereas AI models can produce varied and context-dependent results, requiring specialized testing approaches. A number of tools are being developed to help enable model testing for example promptfoo, deepeval, and deepchecks. In this module we will use one of these tools "promptfoo" to test the consistency of model responses.

In this lab we're going to use a trained model which has been fine tuned with Parasol Insurance specific data.  We're going to write some tests which will use predefined prompts which should return accurate responses demonstrating the trained model.

To test the responses to ensure they are accurate we're going to use another model (an embedding model) to test for similarity between two sentences.

NOTE: Embedding is a means of representing objects like text, images and audio as points in a continuous vector space where the locations of those points in space are semantically meaningful to machine learning (ML) algorithms. Embedding enables machine learning models to find similar objects.  Our tests will use an embedding model served by OpenShift AI to compare two strings semantically to ensure there are similar enough to pass a threshold.

:imagesdir: ../assets/images
:sectnums:

IMPORTANT: If you haven't accessed *Red Hat Developer Hub and Red Hat Dev Spaces* yet, complete the following sections. Otherwise, *proceed* to the <<skip-prereqs, Model Testing Overview>> section.

include::partial-devhub-pre-req.adoc[]
include::partial-dev-spaces-pre-req.adoc[]


[#skip-prereqs]

== Model Testing Overview

As we mentioned in the goals section, we're going to use two models to perform these tests. The configuration for these models can be found in `.vscode/settings.json`.  Open this file and examine it's contents, it should look like this:

[.console-output]
[source,json]
----
{
    "jest": {
        "nodeEnv": {
            "PARASOL_MODEL_URL": "http://parasol-chat-predictor.aiworkshop.svc.cluster.local:8080/v1/chat/completions",
            "EMBEDDING_MODEL_URL": "http://parasol-embedding-predictor.aiworkshop.svc.cluster.local:8080/v1",
            "PARASOL_MODEL_NAME": "parasol-chat"
        },
        "rootPath": "prompt-testing"
    }
}
----

There are a three things to take note of in this JSON file.

* *PARASOL_MODEL_URL*: This is the url of the pre-trained model with knowledge of the Parasol inurance company and it's products.  This model is hosted by OpenShift AI.  Because we're accessing the model from an application running on OpenShift Dev Spaces, we are able to use the internal OpenShift route to connect.
* *PARASOL_MODEL_NAME*: The name of the model as configured in OpenShift AI
* *EMBEDDING_MODEL_URL*: This is the url of the embedding model hosted by OpenShift AI.  The embedding model is used by our tests to compare the response from the trained model with a pre-defined example for similarity.

== The initial set of tests

We have included an initial set of tests which should pass when run against the *Parasol Chat* model.  

For each of these tests we're provided an expected response, so for example, when we ask the question: *"In one sentence, who founded Parasol Insurance?"* we should get a response similar to: *"Parasol Insurance was founded in 1936 by James Falkner and James Labocki."*

This test is described using:

[.console-output]
[source,javascript]
----
  test('should pass when model is asked who founded Parasol insurance', async () => {
    await expect(await callModel('In one sentence, who founded Parasol Insurance? ')).toMatchSemanticSimilarity(
      "Parasol Insurance was founded in 1936 by James Falkner and James Labocki.",
      0.9
    );
  }, 10000);
----

If you take a look at the code above, you can see we are calling the model with the Asynchronous function, *callModel*.

We are then testing the response from the model to ensure it matches semantic similarity a given sample response using *toMatchSemanticSimilarity*.

The semantic similarity between the response and the example response needs to be above *0.9* (with a similiarity of 1 being an exact match)

== Install Node modules

Before we get started running these tests, we need to install the required NPM modules, to do this, follow these instructions.

Open a Terminal window in your OpenShift Dev Spaces workspace.

image::devhub/new-terminal.png[new-terminal]

Enter the following commands:

[.console-input]
[source,bash,subs="+attributes,macros+"]
----
cd prompt-testing
npm install
----

The output should be similar to the following.

[.console-output]
[source,bash]
----
npm WARN skipping integrity check for git dependency ssh://git@github.com/kenlimmj/rouge.git 
npm WARN deprecated glob@7.2.3: Glob versions prior to v9 are no longer supported
npm WARN deprecated inflight@1.0.6: This module is not supported, and leaks memory. Do not use it. Check out lru-cache if you want a good and tested way to coalesce async requests by a key value, which is much more comprehensive and powerful.

added 773 packages, and audited 774 packages in 1m

82 packages are looking for funding
  run `npm fund` for details

4 moderate severity vulnerabilities

To address all issues (including breaking changes), run:
  npm audit fix --force

Run `npm audit` for details.
----

== Run tests

We can now run the tests to ensure they pass.

Open the `prompt-testing/index.test.js` file in the VSCode Explorer.

Then, click on the `Testing` icon On the left hand panel.

image::model-testing/testing-panel.png[npm_output]

Click on the `Run Tests` icon at the top as shown below.

image::model-testing/run-tests.png[run tests]

When the test run is complete you should see.

image::model-testing/run-complete.png[run complete, 800]

Both of the tests should have passed successfully.  

== Failing the tests

Let's try a scenario where we're using a model which has not been trained with Parasol Insurance specific data.

As we discussed above, one of our tests is testing responses to the question: *"Who founded Parasol Insurance?"*.

To answer this question our model has been trained with knowledge about the history of Parasol Insurance.

Let's change the model our tests are performed against to an untrained model (*merlinite-7b-lab*).  To do this, open the `.vscode/settings.json` file to `replace` the contents with:

[.console-input]
[source,json,subs="+attributes,macros+"]
----
{
    "jest": {
        "nodeEnv": {
            "PARASOL_MODEL_URL": "http://granite-3.0-8b-instruct-predictor.aiworkshop.svc.cluster.local:8080/v1/chat/completions",
            "EMBEDDING_MODEL_URL": "http://parasol-embedding-predictor.aiworkshop.svc.cluster.local:8080/v1",
            "PARASOL_MODEL_NAME": "granite-3.0-8b-instruct"
        },
        "rootPath": "prompt-testing"
    }
}
----

Go back to the `Testing` panel. Then, click on the `Run Tests` to run the tests again.  

image::model-testing/run-tests.png[run tests]

You should now see one of the tests failing.

image::model-testing/fail-tests.png[failing tests]

IMPORTANT: Before we move on, replace the `.vscode/settings.json` file with the original contents below:

[.console-input]
[source,json,subs="+attributes,macros+"]
----
{
    "jest": {
        "nodeEnv": {
            "PARASOL_MODEL_URL": "http://parasol-chat-predictor.aiworkshop.svc.cluster.local:8080/v1/chat/completions",
            "EMBEDDING_MODEL_URL": "http://parasol-embedding-predictor.aiworkshop.svc.cluster.local:8080/v1",
            "PARASOL_MODEL_NAME": "parasol-chat"
        },
        "rootPath": "prompt-testing"
    }
}
----

== Adding a new test

We're now going to add a new test to test some new functionality we've added to our model.

Our model has recently been trained to understand the *Apex plus* policy option which will cover drivers for loss of income even if they are at fault.

To test this we're going to ask our model:

[.console-output]
[source,text]
----
Does Parasol insurance policies include loss of income cover if the insured driver is at fault?

----

We expect the response from this question to be similar to the following, mentioning the *Apex plus* policy.

[.console-output]
[source,text]
----
No, Parasol insurance policies do not include loss of income cover if the insured driver is at fault. However, Parasol does offer additional coverage options, such as Apex plus package, which provides reimbursement for loss of income if the policyholder is at fault.
----

To create this test, open the `index.test.js` file. Then, add the following code below this comment, `// ADD NEW TEST HERE`.

[.console-input]
[source,javascript,subs="+attributes,macros+"]
----

describe('Parasol Claim response test', () => {
  const prompt = 'Does Parasol insurance policies include loss of income cover if the insured driver is at fault?';

  const response = 'No, Parasol insurance policies do not include loss of income cover if the insured driver is at fault. However, Parasol does offer additional coverage options, such as Apex plus package, which provides reimbursement for loss of income if the policyholder is at fault.';

  test('should pass when response to policy question is accurate', async () => {
    await expect(await callModel(prompt)).toMatchSemanticSimilarity(
      response,
      0.8
    );
  }, 20000);
});

----

Let's test the model again by clicking on the `Run Tests` button.

image::model-testing/run-tests-2.png[npm_output]

When the test run is complete you should see.

image::model-testing/run-complete-2.png[npm_output, 800]

== Conclusion

We hope you have enjoyed this module!

Here is a quick summary of what we have learned:

- Responses from LLMs are not deterministic, we need to use different techniques to test LLM integrations. Tools are available to help write this tests for example: Promptfoo.
- Embedding models can be used to test for the similarity of strings.  For LLM testing we can use an embedding model to ensure responses are similar to an expected response within a threshold.
- How to add a new test to ensure an LLM response from a customer claim query is similar to an expected example.