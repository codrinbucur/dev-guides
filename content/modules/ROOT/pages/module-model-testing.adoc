= Model Testing - OO minutes
:imagesdir: ../assets/images

++++
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3HTRSDJ3M4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-3HTRSDJ3M4');
</script>
++++

== Goals of this lab

Parasol's developers are always expected to thoroughly test their code. In this exercise, you will explore the unique challenges and techniques involved in testing AI models and applications, which differ significantly from traditional enterprise applications. Traditional apps often have deterministic outputs, whereas AI models can produce varied and context-dependent results, requiring specialized testing approaches. Through this hands-on exercise, use specialized testing tools for validating prompt responses. You will identify and fix failing tests by modifying prompts to ensure data consistency, and validate the fixes by re-running the tests. Finally, you'll add a new test to further ensure the robustness of your prompt responses. This exercise will deepen your understanding of AI testing practices and enhance your ability to maintain reliable AI applications.

== 1. Run podman desktop

Introduction to gen AI + discover and experiment with gen AI models and AI applications on the local desktop, in an inner loop

== 1.1. TBD

=== 1.1.1. TBD

== 2. Start a playground, chat with it

== 3. Kill playground, try text summarization recipe, upload claim PDF, view summarization

== 4. Open summarization app (python) in vscode, inspect code (briefly)

== 5. Change the prompt, restart, and observe changes.

== Conclusion

We hope you have enjoyed this module!

Here is a quick summary of what we have learned:

- TBD
- TBD
- TBD