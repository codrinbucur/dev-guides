= Model Testing - OO minutes
:imagesdir: ../assets/images
:sectnums:

++++
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3HTRSDJ3M4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-3HTRSDJ3M4');
</script>
++++

== Goals of this lab

Parasol's developers are always expected to thoroughly test their code. In this exercise, you will explore the unique challenges and techniques involved in testing AI models and applications, which differ significantly from traditional enterprise applications. Traditional apps often have deterministic outputs, whereas AI models can produce varied and context-dependent results, requiring specialized testing approaches. A number of tools are being developed to help enable model testing for example promptfoo, deepeval, and deepchecks. In this module we will use one of these tools "promptfoo" to test the consistency of model responses.

In this lab we're going to use a trained model which has been fine tuned with Parasol Insurance specific data.  We're going to write some tests which will use predefined prompts which should return accurate responses demonstrating the trained model.

To test the responses to ensure they are accurate we're going to use another model (an embedding model) to test for similarity between two sentences.

NOTE: Embedding is a means of representing objects like text, images and audio as points in a continuous vector space where the locations of those points in space are semantically meaningful to machine learning (ML) algorithms. Embedding enables machine learning models to find similar objects.  Our tests will use an embedding model served by OpenShift AI to compare two strings semantically to ensure there are similar enough to pass a threshold.

:imagesdir: ../assets/images
:sectnums:

IMPORTANT: If you haven't accessed *Red Hat Developer Hub and Red Hat Dev Spaces* yet, complete the following sections. Otherwise, *proceed* to the <<skip-prereqs, Model Testing Overview>> section.

include::partial-devhub-pre-req.adoc[]
include::partial-dev-spaces-pre-req.adoc[]


[#skip-prereqs]

== Model Testing Overview

As we mentioned in the goals section, we're going to use two models to perform these tests. The configuration for these models can be found in .vscode/settings.json.  Open this file and examine it's contents, it should look like this:

[source,json,subs="attributes"]
----
{
    "jest": {
        "nodeEnv": {
            "PARASOL_MODEL_URL": "http://parasol-chat-predictor.aiworkshop.svc.cluster.local:8080/v1/chat/completions",
            "EMBEDDING_MODEL_URL": "http://parasol-embedding-predictor.aiworkshop.svc.cluster.local:8080/v1",
            "PARASOL_MODEL_NAME": "parasol-chat"
        },
        "rootPath": "prompt-testing"
    }
}
----

There are a three things to take note of in this JSON file.

1. "PARASOL_MODEL_URL": This is the url of the pre-trained model with knowledge of the Parasol inurance company and it's products.  This model is hosted by OpenShift AI.  Because we're accessing the model from an application running on OpenShift Dev Spaces, we are able to use the internal OpenShift route to connect.
2. "PARASOL_MODEL_NAME": The name of the model as configured in OpenShift AI
3. "EMBEDDING_MODEL_URL": This is the url of the embedding model hosted by OpenShift AI.  The embedding model is used by our tests to compare the response from the trained model with a pre-defined example for similarity.

== The initial set of tests

We have included an initial set of tests which should pass when run against the "Parasol Chat" model.  

For each of these tests we're provided an expected response, so for example, when we ask the question: "In one sentence, who founded Parasol Insurance?" we should get a response similar to: "Parasol Insurance was founded in 1936 by James Falkner and James Labocki."

This test is described using:

[source,javascript]
----
  test('should pass when model is asked who founded Parasol insurance', async () => {
    await expect(await callModel('In one sentence, who founded Parasol Insurance? ')).toMatchSemanticSimilarity(
      "Parasol Insurance was founded in 1936 by James Falkner and James Labocki.",
      0.9
    );
  }, 10000);
----

If you take a look at the code above, you can see we are calling the model with the Asynchronous function "callModel".

We are then testing the response from the model to ensure it matches semantic similarity a given sample response using "toMatchSemanticSimilarity".

The semantic similarity between the response and the example response needs to be above 0.9 (with a similiarity of 1 being an exact match)

== Install Node modules

Before we get started running these tests, we need to install the required NPM modules, to do this, follow these instructions.

Open a Terminal window in your OpenShift Dev Spaces workspace.

image::devhub/new-terminal.png[new-terminal, 500]

Enter the following commands:

[source,bash,role="copypaste"]
----
cd prompt-testing
npm install
----

Once this command has completed you should see the following.

image::model-testing/npm-output.png[npm_output, 800]

== Run tests

We can now run the tests to ensure they pass.

In the VSCode Explorer, navigate to `prompt-testing/index.test.js`

On the left hand panel, click on the "Testing" icon

image::model-testing/testing-panel.png[npm_output, 800]

Click on the "Run Tests" icon at the top as shown below

image::model-testing/run-tests.png[run tests, 800]

When the test run is complete you should see

image::model-testing/run-complete.png[run complete, 800]

Both of the tests should have passed successfully.  

== Failing the tests

Let's try a scenario where we're using a model which has not been trained with Parasol Insurance specific data.

As we discussed above, one of our tests is testing responses to the question: "Who founded Parasol Insurance?".

In order to answer this question our model has been trained with knowledge about the history of Parasol Insurance.

Let's change the model our tests are performed against to an untrained model (merlinite-7b-lab).  To do this, open the file .vscode/settings.json and replace the contents with:

[source,json,role="copypaste"]
----
{
    "jest": {
        "nodeEnv": {
            "PARASOL_MODEL_URL": "http://merlinite-7b-lab-predictor.aiworkshop.svc.cluster.local:8080/v1/chat/completions",
            "EMBEDDING_MODEL_URL": "http://parasol-embedding-predictor.aiworkshop.svc.cluster.local:8080/v1",
            "PARASOL_MODEL_NAME": "merlinite-7b-lab"
        },
        "rootPath": "prompt-testing"
    }
}
----

On the left hand panel, click on the "Testing" icon.

Click on the "Run Tests" icon as we did previously to re-run the tests.  

image::model-testing/run-tests.png[run tests, 800]

You should now see one of the tests failing.

image::model-testing/fail-tests.png[failing tests, 800]

IMPORTANT: Before we move on, replace the .vscode/settings.json file with the original contents below:

[source,json,role="copypaste"]
----
{
    "jest": {
        "nodeEnv": {
            "PARASOL_MODEL_URL": "http://parasol-chat-predictor.aiworkshop.svc.cluster.local:8080/v1/chat/completions",
            "EMBEDDING_MODEL_URL": "http://parasol-embedding-predictor.aiworkshop.svc.cluster.local:8080/v1",
            "PARASOL_MODEL_NAME": "parasol-chat"
        },
        "rootPath": "prompt-testing"
    }
}
----

== Adding a new test

We're now going to add a new test to test some new functionality we've added to our model.

Our model has recently been trained to understand the "Apex plus" policy option which will cover drivers for loss of income even if they are at fault.

To test this we're going to ask our model:

[source,text]
----
Does Parasol insurance policies include loss of income cover if the insured driver is at fault?

----

We expect the response from this question to be similar to the following, mentioning the "Apex plus" policy.

[source, text]
----
No, Parasol insurance policies do not include loss of income cover if the insured driver is at fault. However, Parasol does offer additional coverage options, such as Apex plus package, which provides reimbursement for loss of income if the policyholder is at fault.
----

To create this test, in the file `index.test.js` add the following to the end replacing the comment: `// ADD NEW TEST HERE`

[source,js,role="copypaste"]
----

describe('Parasol Claim response test', () => {
  const prompt = 'Does Parasol insurance policies include loss of income cover if the insured driver is at fault?';

  const response = 'No, Parasol insurance policies do not include loss of income cover if the insured driver is at fault. However, Parasol does offer additional coverage options, such as Apex plus package, which provides reimbursement for loss of income if the policyholder is at fault.';

  test('should pass when response to policy question is accurate', async () => {
    await expect(await callModel(prompt)).toMatchSemanticSimilarity(
      response,
      0.8
    );
  }, 20000);
});

----

On the left hand panel, click on the "Testing" icon, you should see our new test is listed but has not yet been run.

Click on the "Run Tests" icon at the top as shown below

image::model-testing/run-tests-2.png[npm_output, 800]

When the test run is complete you should see

image::model-testing/run-complete-2.png[npm_output, 800]

== Conclusion

We hope you have enjoyed this module!

Here is a quick summary of what we have learned:

- Responses from LLMs are not deterministic, we need to use different techniques to test LLM integrations. Tools are available to help write this tests for example: Promptfoo.
- Embedding models can be used to test for the similarity of strings.  For LLM testing we can use an embedding model to ensure responses are similar to an expected response within a threshold.
- How to add a new test to ensure an LLM response from a customer claim query is similar to an expected example.


