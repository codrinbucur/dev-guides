= Prompting Basics
:imagesdir: ../assets/images
:sectnums:

++++
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3HTRSDJ3M4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-3HTRSDJ3M4');
</script>
++++

== Goals of this lab

Parasol has had mixed success in the past with generative AI, and believes that better prompts can improve their results. As an AI model builder, it is your job to build in the prompts that will be combined with user input to query the custom Parasol models. 

In this exercise you as an AI developer will delve into the art of crafting effective prompts in order to incorporate Parasol's customized generative AI models into business applications you support. Recognizing the importance of precise and well-structured prompts, this exercise aims to equip you with the best practices to optimize user interactions with custom AI models. 

You'll start by seeing the different ways to experiment with prompt engineering around the platform and then move into understanding with the inputs that steer the LLM towards producing needed value. From there, you'll experiment with various prompt types, including zero-shot, few-shot, and chain-of-thought prompts, using examples to showcase the reasoning capabilities of the language model. Additionally, you'll use these prompts to make a business decision and generate emails that meet specific content, formatting, and tone requirements. 

This hands-on exercise will empower you to refine your prompting techniques, ultimately enhancing the overall effectiveness of Parasol's AI initiatives.

== Chat Playgrounds

There are several ways that you as the AI application developer may interface a particular LLM.  We will briefly review a few of these here and then make a recommendation for you to follow with the subsequent steps of this module.

Before jumping into specific tools, let's review the basics around interfacing with an LLM through a chat or agentic experience.  Since LLMs can often support a wide variety of use cases and personas, it is critical that the LLM receive upfront guidance to define its objectives, constraints, and persona.  These instructions are provided in Natural Language form and is called the "System Prompt".  Once a System Prompt is defined and a chat session begins, the System Prompt cannot be changed.  Subsequent interactions between the LLM and the end user, whether a human or even another system, are called Messages.

Next, depending on use cases, it may or may not be important for the LLM to produce a more predictable response and at other times, a more creative response is needed.  This is controlled through a parameter called Temperature.  Temperature is a floating point number between 0 and 1, where 0 is more predictable and 1 is more creative.  Many tools simply use 0.8 as a default.  Please note that for various technical reasons, even with a Temperature of 0 LLMs will not produce 100% consistently repeatable results.  

Lastly, while experimenting with LLMs, especially from machines without a GPU, it is recommended to constrain the LLM from producing overly verbose responses by setting the Max Tokens.  This coaches the LLM to be more concise with its responses, which can be helpful during testing.

=== Podman Desktop with AI Lab

Podman Desktop is an open source tool that enables you to easily work with containerized applications in an easy to use graphical user interface that is local to your machine.  It is built upon an extensible architecture and AI Lab provides tooling for interacting with Large Language Models in an easy to use, local environment.  The AI Lab extension provides a Playground feature where you can easily provide a system prompt and then have an interactive conversation with the LLM.  The most common parameters, such as temperature and token count, are easily configured within the playground UI itself.

image::prompt/podman-desktop-with-ai-lab.png[Podman Desktop with AI Lab]

If you would like to install Podman Desktop in your own environment, visit podman-desktop.io and click Download Now!  Once installed, click the download button next to AI Lab on either the home screen or search for it in the Extensions view.

=== Quarkus Dev UI with LangChain4j Chat Extension

We are using Quarkus and LainChain4j extensively throughout this exercise, and it provides a user interface for testing system and user prompts.  Dev UI has been preconfigured to communicate with a Parasol Insurance trained LLM exposed through an OpenAI-compliant API.

image::prompt/devui-langchain4j-chat.png[Quarkus Dev UI with LangChain4j Chat]

System Prompt is entered with new chat sessions using the System message text field.  To change the default value, inheritted from the last session, click within the text box and replace the existing value with your guidance.  User Messages are entered in the Message text box at the bottom of the screen.

Temperature and Max Tokens must be changed in the Configuration screen.  Use the Search bar in configuration and search for the following, respectively, to change the default values:

*Adjust Temperature through the Dev UI Configuration View*: 

[.console-input]
[source,text,subs="+attributes,macros+"]
----
openai.parasol-chat.chat-model.temperature
----

image::prompt/devui-change-temperature.png[Change Temperature with Dev UI Chat Playground]

*Adjust Max Tokens through the Dev UI Configuration View*:

[.console-input]
[source,text,subs="+attributes,macros+"]
----
openai.chat-model.max-tokens`
----

image::prompt/devui-change-max-tokens.png[Change Max Tokens with Dev UI Chat Playground]

=== Preferred Chat UI for the module

For the remainder of the lab, we will be assuming you are using the Quarkus Dev UI with the LangChain4j Chat extension.


== Chat with the Model

For this section we will be exercising the model with some basic prompts to gain experience with the tooling, before moving on to more advanced and innovative asks of you.

=== Open Dev UI with LangChain4j Chat

From your parasol-app project cloned to your local machine during a prior module, run the following command from the cli:

`./mvnw quarkus:dev`

Open a browser tab to http://localhost:8080/q/dev-ui/extensions[^]

Click on "Chat".  This is highlighted with a red box in the screenshot below.

image::prompt/podman-extensions.png[Quarkus Dev UI Extensions View]

=== Enter the System Prompt

As we reviewed before, prompting is as important to getting value from a model as training.  Prompting begins with a system prompt, which is where we recommend personas, context and guardrails be established.  The System Prompt is a required field in AI Lab.  Since our initial project for Parasol Insurance is focused on policy rules and regulatory requirements, let's go ahead and set that context here.

* Click the "Start a new conversation" button
* Click in the "System message" text box and replace its contents with the following:

[.console-input]
[source,text,subs="+attributes,macros+"]
----
You are an insurance expert at Parasol Insurance who specializes in rules and regulations as they relate to insurance policies and coverages.  
You are expected to be concise, emphathetic to the user or customer, and answer questions with responses grounded in facts specifically related to Parasol Insurance.  
Do not answer questions unrelated to insurance or Parasol Insurance.
----

* Hit Enter

=== Chat with the LLM

Now, let's use a simple prompt that we know uses data that was included in the previous fine tuning exercises.

* Enter the following text into the Message text box at the very bottom of the page.

`Is rental car coverage included in the most basic vehicle insurance policy?`

* Click the "Send" button.

The response will be negative or a direct No.

image::prompt/basic-question.png[Basic Question and Response]

=== Propigation of Chat History

Chat platgrounds in AI Lab carry the questions you previously asked with you ask further questions are asked.  This context will help the LLM provide more meaningful responses or continue with implied context for follow on inqueries.  Now ask the following question in the same chat session:

`How much does it cost?`

The LLM will now respond with what factors influence the cost of adding rental car insurance.  Notice that the system was able to infer the meaning of "it".  Also notice that it didn't provide a specific dollar value as it relates to the customer inquiry.  Numbers, calculations, math and data specific context (like that associated with a specific customer) are specific challenges we will look at later.  For now, we are focusing on more general usage of the LLM.

image::prompt/basic-question-with-followup.png[Response to Follow Up Question]

=== Ignoring Non-Insurance Related Questions

Now, let's see if the LLM is still adhering to our guidance in the system prompt.  Ask it the following question about tennis shoes and confirm that it declines to comment:

'What Nike tennis shoes are most popular with teenagers?'

The LLM should decline to answer the question.

image::prompt/decline-to-answer.png[LLM Declines to Answer Question]

=== Have Some Fun!

Take some time to ask the LLM various questions and see how it responds.  Remember, chat history influences the next response so start new playgrounds as needed for a fresh start.

//== Try the Summarization Recipe on an Insurance Claimed
//
//=== Start the Summarization Recipe
//
//* In Podman Desktop, go to AI Lab and click on Recipes Catalog
//* Click Summarizer
// - Notice that the Granite foundational model is selected instead of the model previously trained.  For this exercise, that is ok.
//* Click Start AI Application and wait for the model to download and the application to start
//* Once it starts, an arrow icon will appear.  Click on it to open the application in Firefox.
//
//=== Download a File to Summarize
//
//* Open a tab in the firefox window that was just opened and visit the following URL:
// - https://github.com/rh-rad-ai-roadshow/parasol-insurance/raw/main/app/src/main/resources/claims/marty-mcfly-auto.pdf
//* Save the file to the Downloads folder in your home directory.  (default location)
//
//=== Summarize an Insurance Claim PDF
//
//* Return to the summarizer tab in FireFox.
//* Click on the Browse Files button.
//* Navigate to where the previous PDF was saved.  (Downloads quick visit button on the left)
//* Choose the Marty McFly Insurance Claim PDF
//
//The process will take a few moments to complete.  When it does, you will see an accurate translation of the doc using clear, easy to follow bullet points that focus on facts contained in the submitted document.
//
//=== Try Other Documents!
//
//Try downloading and submitting your own documents to experiment with performance.
//
//Keep in mind this is an insecure business server and confidential data should be avoided, along with offensive material.  Also, keep in mind maximum length guidance as provided in the UI.

== Solve a Business Problem with a Prompt!

=== Use Case and Requirements

Customers frequently reach out to our customer service team to discuss their insurance claims.  These emails are very diverse and, while we can easily route them to the claims team, determining how to process from there is time consuming manual labor that Parasol Insurance would like to repurpose elsewhere within the company.  This is where you come in - Parasol Insurance leadership is eager to pilot Generative AI in the context of claims email routing.  It hopes this automation will allow it to move these claims support resources to the claims analysis team, who is getting behind in their processes and causing customer impacting delays.

As the Model Builder, you have already completed training of a robust LLM that is are of Parasol Insurance's business rules.  We will use this model to analyze customer emails and determine how to best handle the customer inquiry.  Architecturally, this logic will be invoked as a simple business service written in Java that generates a simple response that guides routing.  The robust analysis must be performed by the LLM and will require solid prompt to produce quality results.

* Input/Output Specification:
 - LLM Input:  "customer email body"
 - LLM Output: { "claimStatus": "RESPONSE" } where RESPONSE is either NEW, EXISTING, UNRELATED, or UNKNOWN

* Requirements:
 - Determine if the customer email is notifying the company of a new insurance claim or asking about an existing one.
 - If the email was misrouted to the claim team for some reason, respond with UNRELATED
 - Non-business emails should be assigned UNKNOWN

=== Prepare for Prompt Experimentation

* Open Podman Desktop AI Lab
* Click on AI Lab
* Click on Playgrounds
* Click on New Playground
* Choose the Parasol Insurance model 
* Click the Create playbround button
* Set the Max Tokens to a definitive value - something smaller like 100-200 tokens

=== Write a prompt!

Think about what we've learned so far with LLMs and ground yourself in clear communication vs the power of AI.  How would you communicate this use case to a new employee with no background at all in insurance or customer service?  How would you communicate what good looks like in a way that is descriptive of a pattern vs rigid rules that must be followed?

I recommend that you start with a text editor in the shared desktop and then paste updates into Podman Desktop, as this will be an interative process.
A hint - the user prompt will always be the customer email with no other surrounding questions or text.  This means your system prompt will contain all of the instructions for this exercise.  Each change to a system prompt requires a new playground session so pasting this in will help simplify the continuous improvement process.

You can write your own sample emails and paste them into the user prompt text box or you can view them in the Parsol Insurance App.  For the latter, open the web application, click on Claims, select any of the claims in the list and emails are included in the Document tab.

If you run into challenges while drafting the prompt, ask your instructor for assistance.

If you finish this exercise early, can you enhance the process with the following:

* If the response is NEW, is there a customer provided policy number that you can include in the response?  If not, can you return an error message with the code saying that more information is needed?
* If the response is EXISTING, is the claim number provided?  If not, can you return an error message with the EXISTING response?
* Enrich the JSON with the additional data fields.

== Update the Parasol Insurance Java App w/New Prompt

=== Objectives

Now that you've developed a working prompt that can help satisfy leadership objectives with its first Generative AI initiative, you need to incorporate the logic into the Parasol Insurance systems and applications.  For this exercise, that involves updating a REST service that provides routing guidance based on a customer email.  You will update this service to include the prompt and then validate its response.

=== Working Prompt

You are a customer service expert in the claims management processes at Parasol Insurance. 

You are the first point of contact for customer emails involving insurance claims. These emails always fall into one of the following categories - 1.) New Claim , 2.) Status Updates or Additional Details Regarding an Existing Claim, or 3.) Unrelated to Insurance Claims. 

Emails that describe a new accident or discuss filing a new claim are always NEW claims. 
 
Emails that are asking for a status update, voicing concerns about taking too long to process, or providing additional details about an accident that were not previously provided are always EXISTING claims. 

Emails that have nothing to do with insurance or claims are always UNRELATED. 

Any email that cannot be categorized into NEW, EXISTING, or UNRELATED must be categorized as UNKNOWN. 

You will be provided the email body in user prompt and will review it to determine which category to which it belongs. You MUST ALWAYS respond with only one of the following words - NEW, EXISTING, UNRELATED, or UNKNOWN. 

Provide this response in JSON format in the following structure: { "claimStatus": "RESPONSE" } where "RESPONSE" contains NEW, EXISTING, UNRELATED, or UNKNOWN. 

=== Integrate Prompt with Application

TODO


== Conclusion

We hope you have enjoyed this module!

Here is a quick summary of what we have learned:

- Learned how to use the Quarkus Dev UI to chat with a trained model
- Explored common use cases for gaining value from an LLM, such as agents and content analysis
- Developed a new prompt from scratch in support of a new business use case
